{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n",
      "Data Loading done...\n",
      "Create Counter...\n",
      "Write to edges file...\n",
      "CPU times: user 2min 12s, sys: 16.2 s, total: 2min 28s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print ('Data Loading...')\n",
    "data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n",
    "dense_matrix = coo_matrix((data_loader['data'], (data_loader['row'], data_loader['col'])), shape=data_loader['shape'], dtype=int).toarray()\n",
    "print ('Data Loading done...')\n",
    "\n",
    "print ('Create Counter...')\n",
    "\n",
    "edges = Counter()\n",
    "\n",
    "for idx_movie in range(dense_matrix.shape[1]):\n",
    "    movie = dense_matrix[:, idx_movie]\n",
    "    idx_actors = np.where(movie == 1)[0]\n",
    "    for comb in combinations(idx_actors, 2):\n",
    "        edges[comb] += 1\n",
    "\n",
    "print ('Write to edges file...')\n",
    "\n",
    "with open('../data_processed/edges/edges.txt', 'w') as fs:\n",
    "    for edge in edges.elements():\n",
    "        if edges[edge] >= 3:\n",
    "            fs.write('%d,%d,%d\\n' % (edge[0], edge[1], edges[edge]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 3min 11s, total: 5min 31s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "G = nx.read_weighted_edgelist('../data_processed/edges/edges.txt', delimiter=',')\n",
    "\n",
    "clique_generator = nx.enumerate_all_cliques(G)\n",
    "\n",
    "curr = next(clique_generator)\n",
    "while len(curr) < 2:\n",
    "    curr = next(clique_generator)\n",
    "\n",
    "with open('../data_actorset/fulldb_actorset_2.csv', 'w') as fs:\n",
    "    while len(curr) == 2:\n",
    "        fs.write(','.join(curr) + '\\n')\n",
    "        curr = next(clique_generator)\n",
    "\n",
    "with open('../data_actorset/fulldb_actorset_3.csv', 'w') as fs:\n",
    "    while len(curr) == 3:\n",
    "        fs.write(','.join(curr) + '\\n')\n",
    "        curr = next(clique_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2-actor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/log",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9f531d1af66f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nfrom multiprocessing import Pool\\nimport csv\\nimport os\\nimport numpy as np\\n\\ndef find_intersect(actorset):\\n    co_acted_matrix_indices = np.array([], dtype=int).reshape(0,2)\\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\\n    row = data_loader['row']\\n    col = data_loader['col']\\n    for actors in actorset:\\n        co_acted = np.intersect1d(col[row==actors[1]], col[row==actors[2]])\\n        idx_arr = np.full((1, len(co_acted)), actors[0])\\n        co_acted_matrix_indices = np.concatenate((co_acted_matrix_indices, np.vstack((idx_arr,co_acted)).T))\\n    return co_acted_matrix_indices\\n\\nif __name__ == '__main__':   \\n    import tensorflow as tf\\n\\n    act_at_least = 3\\n    top_k = 15\\n\\n    # casting data\\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\\n\\n    # actorset\\n    with open('../data_actorset/fulldb_actorset_2.csv') as fs:\\n        reader = csv.reader(fs, delimiter=',')\\n        actorset = list(reader)\\n        actorset = [[int(b) for b in a] for a in actorset]\\n\\n    t_actorset = tf.constant(actorset)\\n    actorset = [[i] + [int(b) for b in a] for i, a in enumerate(actorset)]\\n\\n    pool = Pool()\\n    chunks = np.array_split(actorset, os.cpu_count())\\n    co_acted_matrix_indices = np.concatenate(pool.map(\\n        find_intersect,\\n        chunks\\n    ))\\n\\n    # Ratings\\n    with open('../data_processed/matrix/mat_ratings.csv') as cfs:\\n        reader = csv.reader(cfs)\\n        ratings = list(reader)\\n    t_ratings = tf.constant(ratings)\\n    t_ratings = tf.string_to_number(ratings)\\n    t_ratings_mask = tf.ones(t_ratings.shape)\\n\\n    t_co_acted_matrix = tf.SparseTensor(\\n        indices=co_acted_matrix_indices, \\n        values=tf.ones(co_acted_matrix_indices.shape[0], dtype=tf.float32), \\n        dense_shape=(len(actorset), data_loader['shape'][1]))\\n\\n    t_product = tf.sparse_tensor_dense_matmul(\\n        t_co_acted_matrix,\\n        t_ratings\\n    )\\n\\n    #count\\n    t_count = tf.sparse_tensor_dense_matmul(\\n        t_co_acted_matrix,\\n        t_ratings_mask\\n    )\\n\\n    #filter out\\n    t_threshold_mask = tf.greater_equal(\\n        t_count,\\n        act_at_least\\n    )\\n\\n    t_masked_product = tf.boolean_mask(t_product, t_threshold_mask)\\n    t_masked_count = tf.boolean_mask(t_count, t_threshold_mask)\\n\\n    #average\\n    t_average = tf.divide(\\n        t_masked_product,\\n        t_masked_count,\\n    )\\n\\n    #top_k\\n    t_top_k_v, t_top_k_i= tf.nn.top_k(\\n        tf.transpose(t_average),\\n        k=top_k,\\n        sorted=False\\n    )\\n\\n    #lookup names\\n\\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_names.npz')\\n    t_names = tf.constant(data_loader['names'])\\n\\n    t_actor_ids = tf.reshape(tf.gather(t_actorset, t_top_k_i), [-1])\\n    t_top_k_names = tf.reshape(tf.gather(\\n        t_names\\n        ,t_actor_ids\\n    ), [-1, int(t_actorset.shape[1])])\\n    # back to normal: tf.reshape(t, [-1, k])\\n\\n    with tf.Session() as sess:\\n        file_writer = tf.summary.FileWriter('/log', sess.graph)\\n        print (t_top_k_v.eval())\\n        print (t_top_k_names.eval())\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDEPRECATED\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUse\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mevent_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/tensorflow/python/summary/writer/event_file_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, max_queue, flush_secs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     self._ev_writer = pywrap_tensorflow.EventsWriter(\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Thomas/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /log"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def find_intersect(actorset):\n",
    "    co_acted_matrix_indices = np.array([], dtype=int).reshape(0,2)\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n",
    "    row = data_loader['row']\n",
    "    col = data_loader['col']\n",
    "    for actors in actorset:\n",
    "        co_acted = np.intersect1d(col[row==actors[1]], col[row==actors[2]])\n",
    "        idx_arr = np.full((1, len(co_acted)), actors[0])\n",
    "        co_acted_matrix_indices = np.concatenate((co_acted_matrix_indices, np.vstack((idx_arr,co_acted)).T))\n",
    "    return co_acted_matrix_indices\n",
    "\n",
    "if __name__ == '__main__':   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    act_at_least = 3\n",
    "    top_k = 15\n",
    "\n",
    "    # casting data\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n",
    "\n",
    "    # actorset\n",
    "    with open('../data_actorset/fulldb_actorset_2.csv') as fs:\n",
    "        reader = csv.reader(fs, delimiter=',')\n",
    "        actorset = list(reader)\n",
    "        actorset = [[int(b) for b in a] for a in actorset]\n",
    "\n",
    "    t_actorset = tf.constant(actorset)\n",
    "    actorset = [[i] + [int(b) for b in a] for i, a in enumerate(actorset)]\n",
    "\n",
    "    pool = Pool()\n",
    "    chunks = np.array_split(actorset, os.cpu_count())\n",
    "    co_acted_matrix_indices = np.concatenate(pool.map(\n",
    "        find_intersect,\n",
    "        chunks\n",
    "    ))\n",
    "\n",
    "    # Ratings\n",
    "    with open('../data_processed/matrix/mat_ratings.csv') as cfs:\n",
    "        reader = csv.reader(cfs)\n",
    "        ratings = list(reader)\n",
    "    t_ratings = tf.constant(ratings)\n",
    "    t_ratings = tf.string_to_number(ratings)\n",
    "    t_ratings_mask = tf.ones(t_ratings.shape)\n",
    "\n",
    "    t_co_acted_matrix = tf.SparseTensor(\n",
    "        indices=co_acted_matrix_indices, \n",
    "        values=tf.ones(co_acted_matrix_indices.shape[0], dtype=tf.float32), \n",
    "        dense_shape=(len(actorset), data_loader['shape'][1]))\n",
    "\n",
    "    t_product = tf.sparse_tensor_dense_matmul(\n",
    "        t_co_acted_matrix,\n",
    "        t_ratings\n",
    "    )\n",
    "\n",
    "    #count\n",
    "    t_count = tf.sparse_tensor_dense_matmul(\n",
    "        t_co_acted_matrix,\n",
    "        t_ratings_mask\n",
    "    )\n",
    "\n",
    "    #filter out\n",
    "    t_threshold_mask = tf.greater_equal(\n",
    "        t_count,\n",
    "        act_at_least\n",
    "    )\n",
    "\n",
    "    t_masked_product = tf.boolean_mask(t_product, t_threshold_mask)\n",
    "    t_masked_count = tf.boolean_mask(t_count, t_threshold_mask)\n",
    "\n",
    "    #average\n",
    "    t_average = tf.divide(\n",
    "        t_masked_product,\n",
    "        t_masked_count,\n",
    "    )\n",
    "\n",
    "    #top_k\n",
    "    t_top_k_v, t_top_k_i= tf.nn.top_k(\n",
    "        tf.transpose(t_average),\n",
    "        k=top_k,\n",
    "        sorted=False\n",
    "    )\n",
    "\n",
    "    #lookup names\n",
    "\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_names.npz')\n",
    "    t_names = tf.constant(data_loader['names'])\n",
    "\n",
    "    t_actor_ids = tf.reshape(tf.gather(t_actorset, t_top_k_i), [-1])\n",
    "    t_top_k_names = tf.reshape(tf.gather(\n",
    "        t_names\n",
    "        ,t_actor_ids\n",
    "    ), [-1, int(t_actorset.shape[1])])\n",
    "    # back to normal: tf.reshape(t, [-1, k])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        file_writer = tf.summary.FileWriter('/log', sess.graph)\n",
    "        print (t_top_k_v.eval())\n",
    "        print (t_top_k_names.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3-actor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.625       9.625       9.625       9.625       9.625       9.63333321\n",
      "  9.625       9.69999981  9.63333321  9.63333321  9.625       9.63333321\n",
      "  9.63333321  9.63333321  9.63333321]\n",
      "[[b'Flavin, James' b'Scannell, Frank J.' b'Sullivan, Brick']\n",
      " [b'Willingham, Travis' b'Inman, Jeremy' b'Clinkenbeard, Colleen']\n",
      " [b'Willingham, Travis' b'Schemmel, Sean' b'Minaguchi, Y\\xc3\\xbbko']\n",
      " [b'Willingham, Travis' b'Boat, David' b'Futterman, Nika']\n",
      " [b'Willingham, Travis' b'Schemmel, Sean' b'Cook, Justin (I)']\n",
      " [b'Altoft, Michael' b'Bracq, Alexander' b'Banks, Richard (VII)']\n",
      " [b'Willingham, Travis' b'Downes, Robin Atkin' b\"O'Shaughnessey, Colleen\"]\n",
      " [b'Billingslea, Beau (I)' b'Baker, Troy (II)' b'Lodge, David (IV)']\n",
      " [b'Moore, Justin D.' b'Blakeney, Derek' b'Lucio, Kelly V.']\n",
      " [b'Moore, Justin D.' b'Humphrey, Alan' b'Dorsainville, Jetto']\n",
      " [b'Willingham, Travis' b'Cox, Chris (I)' b'Mathis III, James']\n",
      " [b'Moore, Justin D.' b'Quintana, Grizelda' b'Borek, Tina']\n",
      " [b'Moore, Justin D.' b'Messer, Casey' b'Stogner, Michael E.']\n",
      " [b'Moore, Justin D.' b'Kenin, Dylan' b'Humphrey, Alan']\n",
      " [b'Moore, Justin D.' b'Douglas, Stafford' b'Dorsainville, Jetto']]\n",
      "CPU times: user 59.3 s, sys: 1min 3s, total: 2min 2s\n",
      "Wall time: 38min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def find_intersect(actorset):\n",
    "    co_acted_matrix_indices = np.array([], dtype=int).reshape(0,2)\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n",
    "    row = data_loader['row']\n",
    "    col = data_loader['col']\n",
    "    for actors in actorset:\n",
    "        co_acted = reduce(np.intersect1d, [col[row==actor] for actor in actors[1:]])\n",
    "        idx_arr = np.full((1, len(co_acted)), actors[0])\n",
    "        co_acted_matrix_indices = np.concatenate((co_acted_matrix_indices, np.vstack((idx_arr,co_acted)).T))\n",
    "    return co_acted_matrix_indices\n",
    "\n",
    "if __name__ == '__main__':   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    act_at_least = 3\n",
    "    top_k = 15\n",
    "\n",
    "    # casting data\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n",
    "\n",
    "    # actorset\n",
    "    with open('../data_actorset/fulldb_actorset_3.csv') as fs:\n",
    "        reader = csv.reader(fs, delimiter=',')\n",
    "        actorset = list(reader)\n",
    "        actorset = [[int(b) for b in a] for a in actorset]\n",
    "\n",
    "    t_actorset = tf.constant(actorset)\n",
    "    actorset = [[i] + [int(b) for b in a] for i, a in enumerate(actorset)]\n",
    "\n",
    "    pool = Pool()\n",
    "    chunks = np.array_split(actorset, os.cpu_count())\n",
    "    co_acted_matrix_indices = np.concatenate(pool.map(\n",
    "        find_intersect,\n",
    "        chunks\n",
    "    ))\n",
    "\n",
    "    # Ratings\n",
    "    with open('../data_processed/matrix/mat_ratings.csv') as cfs:\n",
    "        reader = csv.reader(cfs)\n",
    "        ratings = list(reader)\n",
    "    t_ratings = tf.constant(ratings)\n",
    "    t_ratings = tf.string_to_number(ratings)\n",
    "    t_ratings_mask = tf.ones(t_ratings.shape)\n",
    "\n",
    "    t_co_acted_matrix = tf.SparseTensor(\n",
    "        indices=co_acted_matrix_indices, \n",
    "        values=tf.ones(co_acted_matrix_indices.shape[0], dtype=tf.float32), \n",
    "        dense_shape=(len(actorset), data_loader['shape'][1]))\n",
    "\n",
    "    t_product = tf.sparse_tensor_dense_matmul(\n",
    "        t_co_acted_matrix,\n",
    "        t_ratings\n",
    "    )\n",
    "\n",
    "    #count\n",
    "    t_count = tf.sparse_tensor_dense_matmul(\n",
    "        t_co_acted_matrix,\n",
    "        t_ratings_mask\n",
    "    )\n",
    "\n",
    "    #filter out\n",
    "    t_threshold_mask = tf.greater_equal(\n",
    "        t_count,\n",
    "        act_at_least\n",
    "    )\n",
    "\n",
    "    t_masked_product = tf.boolean_mask(t_product, t_threshold_mask)\n",
    "    t_masked_count = tf.boolean_mask(t_count, t_threshold_mask)\n",
    "\n",
    "    #average\n",
    "    t_average = tf.divide(\n",
    "        t_masked_product,\n",
    "        t_masked_count,\n",
    "    )\n",
    "\n",
    "    #top_k\n",
    "    t_top_k_v, t_top_k_i= tf.nn.top_k(\n",
    "        tf.transpose(t_average),\n",
    "        k=top_k,\n",
    "        sorted=False\n",
    "    )\n",
    "\n",
    "    #lookup names\n",
    "\n",
    "    data_loader = np.load('../data_processed/filtered_matrix/filtered_names.npz')\n",
    "    t_names = tf.constant(data_loader['names'])\n",
    "\n",
    "    t_actor_ids = tf.reshape(tf.gather(t_actorset, t_top_k_i), [-1])\n",
    "    t_top_k_names = tf.reshape(tf.gather(\n",
    "        t_names\n",
    "        ,t_actor_ids\n",
    "    ), [-1, int(t_actorset.shape[1])])\n",
    "    # back to normal: tf.reshape(t, [-1, k])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print (t_top_k_v.eval())\n",
    "        print (t_top_k_names.eval())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
