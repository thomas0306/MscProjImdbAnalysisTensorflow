
# coding: utf-8

# In[7]:

import numpy as np
from scipy.sparse import coo_matrix
from itertools import combinations
from collections import Counter
import networkx as nx
import tensorflow as tf


# In[5]:

get_ipython().run_cell_magic('time', '', "\nprint ('Data Loading...')\ndata_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\ndense_matrix = coo_matrix((data_loader['data'], (data_loader['row'], data_loader['col'])), shape=data_loader['shape'], dtype=int).toarray()\nprint ('Data Loading done...')\n\nprint ('Create Counter...')\n\nedges = Counter()\n\nfor idx_movie in range(dense_matrix.shape[1]):\n    movie = dense_matrix[:, idx_movie]\n    idx_actors = np.where(movie == 1)[0]\n    for comb in combinations(idx_actors, 2):\n        edges[comb] += 1\n\nprint ('Write to edges file...')\n\nwith open('../data_processed/edges/edges.txt', 'w') as fs:\n    for edge in edges.elements():\n        if edges[edge] >= 3:\n            fs.write('%d,%d,%d\\n' % (edge[0], edge[1], edges[edge]))")


# In[2]:

get_ipython().run_cell_magic('time', '', "\nG = nx.read_weighted_edgelist('../data_processed/edges/edges.txt', delimiter=',')\n\nclique_generator = nx.enumerate_all_cliques(G)\n\ncurr = next(clique_generator)\nwhile len(curr) < 2:\n    curr = next(clique_generator)\n\nwith open('../data_actorset/fulldb_actorset_2.csv', 'w') as fs:\n    while len(curr) == 2:\n        fs.write(','.join(curr) + '\\n')\n        curr = next(clique_generator)\n\nwith open('../data_actorset/fulldb_actorset_3.csv', 'w') as fs:\n    while len(curr) == 3:\n        fs.write(','.join(curr) + '\\n')\n        curr = next(clique_generator)")


# ## 2-actor set

# In[3]:

get_ipython().run_cell_magic('time', '', "\nfrom multiprocessing import Pool\nimport csv\nimport os\nimport numpy as np\n\ndef find_intersect(actorset):\n    co_acted_matrix_indices = np.array([], dtype=int).reshape(0,2)\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n    row = data_loader['row']\n    col = data_loader['col']\n    for actors in actorset:\n        co_acted = np.intersect1d(col[row==actors[1]], col[row==actors[2]])\n        idx_arr = np.full((1, len(co_acted)), actors[0])\n        co_acted_matrix_indices = np.concatenate((co_acted_matrix_indices, np.vstack((idx_arr,co_acted)).T))\n    return co_acted_matrix_indices\n\nif __name__ == '__main__':   \n    import tensorflow as tf\n\n    act_at_least = 3\n    top_k = 15\n\n    # casting data\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n\n    # actorset\n    with open('../data_actorset/fulldb_actorset_2.csv') as fs:\n        reader = csv.reader(fs, delimiter=',')\n        actorset = list(reader)\n        actorset = [[int(b) for b in a] for a in actorset]\n\n    t_actorset = tf.constant(actorset)\n    actorset = [[i] + [int(b) for b in a] for i, a in enumerate(actorset)]\n\n    pool = Pool()\n    chunks = np.array_split(actorset, os.cpu_count())\n    co_acted_matrix_indices = np.concatenate(pool.map(\n        find_intersect,\n        chunks\n    ))\n\n    # Ratings\n    with open('../data_processed/matrix/mat_ratings.csv') as cfs:\n        reader = csv.reader(cfs)\n        ratings = list(reader)\n    t_ratings = tf.constant(ratings)\n    t_ratings = tf.string_to_number(ratings)\n    t_ratings_mask = tf.ones(t_ratings.shape)\n\n    t_co_acted_matrix = tf.SparseTensor(\n        indices=co_acted_matrix_indices, \n        values=tf.ones(co_acted_matrix_indices.shape[0], dtype=tf.float32), \n        dense_shape=(len(actorset), data_loader['shape'][1]))\n\n    t_product = tf.sparse_tensor_dense_matmul(\n        t_co_acted_matrix,\n        t_ratings\n    )\n\n    #count\n    t_count = tf.sparse_tensor_dense_matmul(\n        t_co_acted_matrix,\n        t_ratings_mask\n    )\n\n    #filter out\n    t_threshold_mask = tf.greater_equal(\n        t_count,\n        act_at_least\n    )\n\n    t_masked_product = tf.boolean_mask(t_product, t_threshold_mask)\n    t_masked_count = tf.boolean_mask(t_count, t_threshold_mask)\n\n    #average\n    t_average = tf.divide(\n        t_masked_product,\n        t_masked_count,\n    )\n\n    #top_k\n    t_top_k_v, t_top_k_i= tf.nn.top_k(\n        tf.transpose(t_average),\n        k=top_k,\n        sorted=False\n    )\n\n    #lookup names\n\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_names.npz')\n    t_names = tf.constant(data_loader['names'])\n\n    t_actor_ids = tf.reshape(tf.gather(t_actorset, t_top_k_i), [-1])\n    t_top_k_names = tf.reshape(tf.gather(\n        t_names\n        ,t_actor_ids\n    ), [-1, int(t_actorset.shape[1])])\n    # back to normal: tf.reshape(t, [-1, k])\n\n    with tf.Session() as sess:\n        file_writer = tf.summary.FileWriter('/log', sess.graph)\n        print (t_top_k_v.eval())\n        print (t_top_k_names.eval())")


# ## 3-actor set

# In[3]:

get_ipython().run_cell_magic('time', '', "\nfrom multiprocessing import Pool\nimport csv\nimport os\nimport numpy as np\nfrom functools import reduce\n\ndef find_intersect(actorset):\n    co_acted_matrix_indices = np.array([], dtype=int).reshape(0,2)\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n    row = data_loader['row']\n    col = data_loader['col']\n    for actors in actorset:\n        co_acted = reduce(np.intersect1d, [col[row==actor] for actor in actors[1:]])\n        idx_arr = np.full((1, len(co_acted)), actors[0])\n        co_acted_matrix_indices = np.concatenate((co_acted_matrix_indices, np.vstack((idx_arr,co_acted)).T))\n    return co_acted_matrix_indices\n\nif __name__ == '__main__':   \n    import tensorflow as tf\n\n    act_at_least = 3\n    top_k = 15\n\n    # casting data\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_actin_data_for_clustering.npz')\n\n    # actorset\n    with open('../data_actorset/fulldb_actorset_3.csv') as fs:\n        reader = csv.reader(fs, delimiter=',')\n        actorset = list(reader)\n        actorset = [[int(b) for b in a] for a in actorset]\n\n    t_actorset = tf.constant(actorset)\n    actorset = [[i] + [int(b) for b in a] for i, a in enumerate(actorset)]\n\n    pool = Pool()\n    chunks = np.array_split(actorset, os.cpu_count())\n    co_acted_matrix_indices = np.concatenate(pool.map(\n        find_intersect,\n        chunks\n    ))\n\n    # Ratings\n    with open('../data_processed/matrix/mat_ratings.csv') as cfs:\n        reader = csv.reader(cfs)\n        ratings = list(reader)\n    t_ratings = tf.constant(ratings)\n    t_ratings = tf.string_to_number(ratings)\n    t_ratings_mask = tf.ones(t_ratings.shape)\n\n    t_co_acted_matrix = tf.SparseTensor(\n        indices=co_acted_matrix_indices, \n        values=tf.ones(co_acted_matrix_indices.shape[0], dtype=tf.float32), \n        dense_shape=(len(actorset), data_loader['shape'][1]))\n\n    t_product = tf.sparse_tensor_dense_matmul(\n        t_co_acted_matrix,\n        t_ratings\n    )\n\n    #count\n    t_count = tf.sparse_tensor_dense_matmul(\n        t_co_acted_matrix,\n        t_ratings_mask\n    )\n\n    #filter out\n    t_threshold_mask = tf.greater_equal(\n        t_count,\n        act_at_least\n    )\n\n    t_masked_product = tf.boolean_mask(t_product, t_threshold_mask)\n    t_masked_count = tf.boolean_mask(t_count, t_threshold_mask)\n\n    #average\n    t_average = tf.divide(\n        t_masked_product,\n        t_masked_count,\n    )\n\n    #top_k\n    t_top_k_v, t_top_k_i= tf.nn.top_k(\n        tf.transpose(t_average),\n        k=top_k,\n        sorted=False\n    )\n\n    #lookup names\n\n    data_loader = np.load('../data_processed/filtered_matrix/filtered_names.npz')\n    t_names = tf.constant(data_loader['names'])\n\n    t_actor_ids = tf.reshape(tf.gather(t_actorset, t_top_k_i), [-1])\n    t_top_k_names = tf.reshape(tf.gather(\n        t_names\n        ,t_actor_ids\n    ), [-1, int(t_actorset.shape[1])])\n    # back to normal: tf.reshape(t, [-1, k])\n\n    with tf.Session() as sess:\n        print (t_top_k_v.eval())\n        print (t_top_k_names.eval())\n")

